ğŸ”¢ Data & Tensors

Tensor: fancy word for an n-dim array (like NumPy arrays but GPU-friendly). Images are typically shaped [C, H, W] = channels, height, width. ğŸ§®ğŸ§ 

pixel_values: the actual image tensor you feed the model. In your pipeline itâ€™s already preprocessed to the right shape. ğŸ–¼ï¸â¡ï¸ğŸ“¦

labels: integers representing classes (e.g., 0=AI, 1=Deepfake, 2=Real). Thatâ€™s what the model tries to predict. ğŸ·ï¸

ğŸ“¦ Datasets, Loaders & Batching

Dataset (custom class): how you tell PyTorch how to get one item (image + label). __getitem__ returns a dict with pixel_values and labels. ğŸ“š

DataLoader: wraps a dataset to give you batches. Handles shuffling, multiprocessing, etc. Batch = multiple samples at once. ğŸ§º

collate_fn: how to stack individual items into a batch. Yours does torch.stack for both images and labels. ğŸ”—

Batch size: how many samples per forward/backward pass. More batch size = more VRAM. ğŸ§Š

ğŸ§  Model Architectures

CNN (Convolutional Neural Network): learns spatial patterns with conv filters; perfect for images. ğŸ“¡

ResNet50: a pretrained CNN with 50 layers & skip connections (residuals) so training deep nets doesnâ€™t explode. You fine-tune it for your 3 classes. ğŸ—ï¸

Pretrained: weights learned on a massive dataset (ImageNet). You start with those instead of random â†’ faster/better learning. ğŸ‹ï¸â€â™€ï¸âš¡

Transfer Learning: reusing a pretrained backbone and only swapping the last layer(s) for your task. Thatâ€™s exactly what youâ€™re doing. ğŸ”

ğŸ§± Common Layers (what they do)

nn.Conv2d: slides learnable filters over the image â†’ extracts edges, textures, patterns. ğŸ§©

nn.BatchNorm2d: normalizes activations per channel â†’ stabilizes training & lets you use bigger learning rates. ğŸ§¼

nn.ReLU: activation function: max(0, x). Adds non-linearity so the model can learn complex stuff. âš¡

nn.MaxPool2d: downsampling by taking local maximums â†’ smaller spatial size, keeps strong features. ğŸ•³ï¸

nn.AdaptiveAvgPool2d((1,1)): global average pooling â†’ compresses each channel to a single value regardless of input size. ğŸ§¯

nn.Linear: fully connected layer â†’ mixes features to produce class logits. ğŸ§®

nn.Dropout: randomly zeroes some activations during training â†’ regularization to reduce overfitting. ğŸ²

ğŸ§­ Forward/Backward Pass

Forward pass: data â†’ model â†’ outputs (logits). ğŸ”œ

Backward pass: compute gradients of loss wrt parameters via backprop; optimizer uses them to update weights. ğŸ”

model.train() vs model.eval():

train() enables dropout & batchnorm updates.

eval() disables dropout & uses running stats for batchnorm. ğŸ›ï¸

with torch.no_grad(): turns off gradient tracking (faster, less memory) during eval/inference. ğŸš«âœï¸

ğŸ¯ Outputs, Losses & Probabilities

Logits: raw outputs from the last linear layer (not probabilities). ğŸ“ˆ

torch.softmax: converts logits â†’ probabilities that sum to 1 across classes. ğŸ§®â¡ï¸ğŸ²

Predicted class: torch.max(logits, dim=1) gives the index of the largest logit â†’ your predicted label. ğŸ†

nn.CrossEntropyLoss: the go-to classification loss for multi-class problems. Combines LogSoftmax + NLLLoss under the hood. ğŸ§¨

ğŸ”§ Optimizer & LR Scheduler

optim.Adam: adaptive optimizer that tweaks learning rates per parameter using running stats of gradients. Usually converges faster than plain SGD. ğŸ› ï¸

Learning Rate (LR): how big each update step is. Too big = chaos, too small = snail. ğŸŒ

StepLR(step_size=5, gamma=0.5): every 5 epochs, multiply LR by 0.5. Helps training settle into minima. ğŸ“‰

optimizer.zero_grad(): clears old gradients before computing new ones. You donâ€™t want gradients snowballing. ğŸ§¹

loss.backward(): compute gradients.

optimizer.step(): apply gradients (update weights). ğŸ§‘â€ğŸ”¬

ğŸ—“ï¸ Training Loop Lingo

Epoch: one full pass over the training set. ğŸ”„

Iteration/step: one batch update inside an epoch. ğŸ‘£

Training vs Validation vs Test:

Training: used to update weights.

Validation: used to tune decisions (like early stopping, LR schedules, model selection).

Test: final unbiased evaluation. ğŸ§ª

random_split: splits your dataset into train/val/test subsets by size. âœ‚ï¸

ğŸ§ª Metrics (how we judge the model)

Accuracy: % of correct predictions. Simple but can be misleading with class imbalance. âœ…/âŒ

Classification Report: per-class precision, recall, F1-score, and support:

Precision: out of predicted class X, how many were actually X (low FP). ğŸ¯

Recall: out of actual class X, how many did we catch (low FN). ğŸ§²

F1: harmonic mean of precision & recall (balance). âš–ï¸

Support: number of true instances for each class. ğŸ”¢

Confusion Matrix: table showing how often each true class was predicted as each class. Great for spotting which classes your model confuses. ğŸ§©

ğŸ§¯ Overfitting & Regularization

Overfitting: model memorizes training set and flops on val/test. Signs: train acc â†‘ while val acc â†“. ğŸ“‰

Regularization tricks youâ€™re using:

Dropout (randomly zero activations)

Pretrained backbones (better generalization)

Validation monitoring (pick best val acc) ğŸ§°

(Optional extras you could add): weight decay (L2), data augmentation, early stopping. ğŸ§ª

ğŸ’¾ Saving / Loading Models

state_dict(): dictionary of model weights.

.pth file: PyTorch weight checkpoint.

You save the best model by validation accuracy, then reload it at the end. ğŸ…

ğŸ§© Misc but Important

device = 'cuda' if torch.cuda.is_available() else 'cpu': trains on GPU if you have one (way faster). âš¡

Parameter count: sum(p.numel() for p in model.parameters()) â†’ how thicc your model is. ğŸ°

Class names: you map indices to names ['AI-generated','Deepfake','Real'] for readable logs. ğŸ·ï¸

scheduler.get_last_lr(): peeks the current LR after scheduling. ğŸ‘€

torch.max(outputs.data, 1): gets predicted class, but fyi using .data is kinda old-school; prefer torch.argmax(outputs, dim=1) these days. ğŸ§“â¡ï¸ğŸ†•

ğŸ§  What your two models imply

DeepfakeClassifier (ResNet50): transfer learning with a pretrained beast â†’ great when your dataset isnâ€™t massive. ğŸ‰

SimpleClassifier: lightweight CNN from scratch â†’ faster to train, good baseline, might underperform on complex data. ğŸª¶

ğŸ§ª Inference (predicting on one sample)

model.eval() + no_grad(): inference mode for speed & stability. ğŸ

softmax then argmax: get probs & top class.

You print predicted class + confidence (probability of that class). ğŸ”®

ğŸ§  Pro Tips (optional upgrades)

Freeze early layers of ResNet at first, train only the head; then unfreeze more layers to fine-tune = often better & stabler. ğŸ¥¶â¡ï¸ğŸ”¥

Data augmentation (random crops, flips, color jitter) boosts robustness, especially for deepfake artifacts. ğŸ›ï¸

Class imbalance? Use weighted loss or samplers if one class (e.g., â€œRealâ€) dominates. âš–ï¸

Early stopping: stop when val loss stops improving. Saves time & prevents overfit. â¹ï¸

Use torch.argmax(outputs, dim=1) instead of .data style. Cleaner & safer. ğŸ§¼
